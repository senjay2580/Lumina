好的，从「事件与回调」开始，详细步骤如下：

------

## 步骤 1：配置事件订阅（飞书开放平台）

**位置：** 左侧菜单 → 「事件与回调」

### 1.1 配置请求地址

在「请求地址配置」区域：

- 请求地址填入：

  ```
  https://lladwhzdcfrvrkbendmw.supabase.co/functions/v1/feishu-webhook
  ```

- 点击「保存」

⚠️ **注意：** 这一步会验证 URL，但我们的 Edge Function 还没部署，所以会失败。**先跳过这一步，等部署完再回来配置。**

### 1.2 复制 Verification Token

在页面上找到「Verification Token」，复制保存（类似 `xxxxxxxxxxxxxx`）

### 1.3 添加事件

点击「添加事件」，搜索并勾选：

- `im.message.receive_v1` - 接收消息

------

## 步骤 2：配置权限（飞书开放平台）

**位置：** 左侧菜单 → 「权限管理」

搜索并开通以下权限：

| 权限                     | 说明                  |
| ------------------------ | --------------------- |
| `im:message`             | 获取与发送消息        |
| `im:message:send_as_bot` | 以机器人身份发消息    |
| `im:resource`            | 获取消息中的文件/图片 |

------

## 步骤 3：获取凭证（飞书开放平台）

**位置：** 左侧菜单 → 「凭证与基础信息」

复制以下信息：

- `App ID`（类似 `cli_xxxxxxxxxx`）
- `App Secret`（类似 `xxxxxxxxxxxxxxxxxxxxxxxx`）

------

## 步骤 4：配置 Supabase 环境变量

**位置：** https://supabase.com/dashboard/project/lladwhzdcfrvrkbendmw/settings/functions

1. 点击左侧「Settings」→「Edge Functions」
2. 找到「Function Secrets」区域
3. 点击「Add new secret」，添加以下三个：

| Name                        | Value                       |
| --------------------------- | --------------------------- |
| `FEISHU_APP_ID`             | 你复制的 App ID             |
| `FEISHU_APP_SECRET`         | 你复制的 App Secret         |
| `FEISHU_VERIFICATION_TOKEN` | 你复制的 Verification Token |

------

## 步骤 5：部署 Edge Function

在项目根目录执行：

```bash
supabase functions deploy feishu-webhook
```

如果没有登录 Supabase CLI，先执行：

```bash
supabase login
supabase link --project-ref lladwhzdcfrvrkbendmw
```

------

## 步骤 6：回到飞书配置请求地址

部署成功后，回到飞书开放平台 → 「事件与回调」：

1. 填入请求地址：`https://lladwhzdcfrvrkbendmw.supabase.co/functions/v1/feishu-webhook`
2. 点击「保存」
3. 这次应该验证成功 ✅

------

## 步骤 7：发布机器人

**位置：** 左侧菜单 → 「版本管理与发布」

1. 点击「创建版本」
2. 填写版本号（如 `1.0.0`）和更新说明
3. 点击「保存」
4. 点击「申请发布」（企业内部应用通常自动通过）

------

## 步骤 8：测试

1. 在飞书中搜索「Lumina 资源助手」
2. 添加机器人
3. 发送「帮助」测试是否正常响应
4. 在 Lumina 设置页面获取绑定码
5. 发送 `/bind 绑定码` 完成绑定
6. 发送一个链接测试添加资源

------



---



# Reddit github 爬取



## 第一步：部署和配置

**1. 执行数据库迁移**

在 Supabase SQL Editor 中运行这个文件：

```
supabase/plus/010_add_prompt_crawler.sql
supabase/plus/024_add_crawl_logs.sql
```

这会创建四个表和日志表。

**2. 配置 API 密钥**

在 Supabase Dashboard 中，找到 Edge Functions > prompt-crawler > Settings，添加环境变量：

```
REDDIT_CLIENT_ID=你的Reddit客户端ID
REDDIT_CLIENT_SECRET=你的Reddit客户端密钥
GITHUB_TOKEN=你的GitHub Token
OPENAI_API_KEY=你的OpenAI API Key
```

怎么获取这些密钥？

- **Reddit**: 访问 https://www.reddit.com/prefs/apps，创建应用，记录 client_id 和 secret
- **GitHub**: 访问 https://github.com/settings/tokens，生成 Personal Access Token
- **OpenAI**: 访问 https://platform.openai.com/api-keys，创建 API Key

**3. 部署 Edge Function**

```bash
supabase functions deploy prompt-crawler
```

------

## 第二步：配置爬取参数

打开应用，进入"提示词采集"页面，点击"配置"标签页。

你会看到这些配置项：

**Reddit 子版块** - 要爬取哪些 Reddit 社区

- 默认值：`["ChatGPT","ChatGPTPro","PromptEngineering","LocalLLaMA","ClaudeAI","OpenAI","Anthropic"]`
- 修改为你感兴趣的社区，比如只要 `["ChatGPT","PromptEngineering"]`

**GitHub 搜索关键词** - 在 GitHub 上搜索什么

- 默认值：`["prompt engineering","chatgpt prompts","llm prompts","awesome prompts","ai prompts"]`
- 可以改成 `["prompt engineering","best practices"]`

**Reddit 最低分数** - 只爬取分数 >= 这个值的帖子

- 默认值：10
- 如果想要更多内容，改成 5；如果只要高质量的，改成 50

**GitHub 最低 Stars** - 只爬取 Stars >= 这个值的仓库

- 默认值：50
- 改成 100 会更严格，改成 20 会更宽松

**AI 质量阈值** - 只保存评分 >= 这个值的提示词

- 默认值：6.0（满分 10 分）
- 改成 7.0 会更严格，改成 5.0 会更宽松

**爬取间隔** - 多少小时爬取一次（如果设置了定时任务）

- 默认值：24（每天一次）

------

## 第三步：启动爬取

**方式一：爬取全部**

点击"立即采集"按钮，系统会同时爬取 Reddit 和 GitHub。

**方式二：只爬取 Reddit**

点击"Reddit"按钮，只爬取 Reddit 的内容。

**方式三：只爬取 GitHub**

点击"GitHub"按钮，只爬取 GitHub 的内容。

点击后，系统会：

1. 创建一条任务记录（status = running）
2. 根据配置爬取内容
3. 用 AI 分析内容，提取提示词
4. 保存结果

------

## 第四步：查看爬取进度

点击"查看进度"按钮，会弹出一个模态框，显示：

**统计信息**

- 发现来源：爬取到了多少个帖子/仓库
- 新增来源：其中有多少是新的（没爬过的）
- 提取提示词：AI 提取了多少个高质量提示词

**详细日志**

- 显示爬取过程中的每一步
- 蓝色 = 信息日志（正常进行）
- 黄色 = 警告日志（有问题但继续了）
- 红色 = 错误日志（出错了）

比如你会看到：

```
[14:30:15] [INFO] Crawl job started
[14:30:16] [INFO] Configuration loaded
[14:30:17] [INFO] Starting Reddit crawl
[14:30:18] [INFO] Crawling Reddit: r/ChatGPT
[14:30:25] [INFO] Found 12 posts in r/ChatGPT
[14:30:26] [INFO] Extracted 8 prompts from Reddit post
...
[14:35:42] [INFO] Crawl job finished successfully
```

------

## 第五步：审核提示词

爬取完成后，进入"待审核"标签页，你会看到 AI 提取的所有提示词。

每个提示词显示：

- **来源标签** - Reddit 或 GitHub
- **分类标签** - AI 建议的分类（编程、写作等）
- **质量评分** - 0-10 分
- **标题** - 提示词的名称
- **内容预览** - 提示词的前两行
- **来源链接** - 点击可查看原始来源

**单个审核**

鼠标悬停在提示词上，会出现三个按钮：

- ✓ 通过 - 审核通过，标记为 `is_approved=true`
- ✗ 删除 - 不要这个提示词，从数据库删除
- ⬇ 导入 - 直接导入到你的提示词库

**批量审核**

1. 点击左上角的"全选"或逐个选择提示词
2. 点击"批量通过"审核所有选中的提示词
3. 选择分类后点击"批量导入"

------

## 第六步：导入到提示词库

**单个导入**

在"待审核"或"已审核"标签页，点击提示词的"导入"按钮。

系统会：

1. 在 `prompts` 表中创建新记录
2. 在 `extracted_prompts` 表中标记 `imported_to_prompt_id`
3. 提示词出现在你的提示词库中

**批量导入**

1. 选择多个提示词
2. 在下拉框中选择目标分类（可选）
3. 点击"批量导入"

------

## 第七步：查看爬取历史

进入"爬取历史"标签页，看到所有历史任务：

| 时间             | 类型   | 状态      | 发现 | 新增 | 提取 | 操作 |
| ---------------- | ------ | --------- | ---- | ---- | ---- | ---- |
| 2024-01-06 14:30 | all    | completed | 120  | 85   | 72   | -    |
| 2024-01-05 14:30 | reddit | failed    | 50   | -    | -    | 重试 |

**状态说明**

- completed - 成功完成
- running - 正在进行
- failed - 失败了
- pending - 待处理

如果任务失败了，点击"重试"按钮重新爬取。

------

## 常见操作场景

**场景 1：我想要更多编程相关的提示词**

1. 进入"配置"标签页
2. 修改 GitHub 搜索关键词，加入 `"python prompts"`, `"javascript prompts"`
3. 降低 GitHub 最低 Stars 从 50 改成 20
4. 点击"立即采集"

**场景 2：我只想要高质量的提示词**

1. 进入"配置"标签页
2. 提高 AI 质量阈值从 6.0 改成 8.0
3. 提高 Reddit 最低分数从 10 改成 50
4. 提高 GitHub 最低 Stars 从 50 改成 200
5. 点击"立即采集"

**场景 3：爬取失败了，我想重试**

1. 进入"爬取历史"标签页
2. 找到状态为 "failed" 的任务
3. 点击"重试"按钮
4. 点击"查看进度"查看详细日志，了解失败原因

**场景 4：我想导入某个提示词到我的库**

1. 进入"待审核"或"已审核"标签页
2. 找到要导入的提示词
3. 点击"导入"按钮
4. 选择分类（可选）
5. 确认导入

**场景 5：我想批量导入所有高质量提示词**

1. 进入"已审核"标签页
2. 点击"全选"
3. 在下拉框选择分类（比如"编程"）
4. 点击"批量导入"
5. 所有提示词都会导入到你的库中

------

## 数据流向总结

```
1. 配置参数 (crawl_config)
   ↓
2. 点击采集按钮
   ↓
3. 创建任务 (crawl_jobs, status=running)
   ↓
4. 爬取内容 (prompt_sources)
   ↓
5. AI 分析提取 (extracted_prompts, is_approved=false)
   ↓
6. 前端显示在"待审核"
   ↓
7. 用户审核通过 (is_approved=true)
   ↓
8. 用户导入 (imported_to_prompt_id 被填充)
   ↓
9. 提示词出现在你的提示词库 (prompts 表)
```

------

## 监控和维护

**定期检查**

- 每周看一次爬取成功率
- 调整配置参数优化结果
- 删除不需要的提示词

**查看成本**

- Reddit API：免费
- GitHub API：免费
- OpenAI：约 $0.001-0.005 per 分析
- 月成本：$3-15（100 条/天）

**清理日志** 如果日志表太大，可以定期清理：

```sql
DELETE FROM crawl_logs 
WHERE created_at < NOW() - INTERVAL '30 days';
```





我来详细解释这些密钥的获取方式和为什么要配置在 Supabase 上。

## 为什么要配置在 Supabase 上？

**核心原因：爬取系统运行在 Supabase Edge Function 上，不是在你的本地或前端**

```
你的应用 (前端)
    ↓
    调用 Edge Function
    ↓
Supabase Edge Function (后端服务器)
    ↓
    需要 API 密钥来调用 Reddit/GitHub/OpenAI
```

Edge Function 是运行在 Supabase 服务器上的后端代码。它需要这些密钥来：

1. 认证到 Reddit API
2. 认证到 GitHub API
3. 调用 OpenAI API

如果密钥放在前端代码里，会被暴露在浏览器中，任何人都能看到。所以必须放在后端（Supabase）。

------

## 如何获取这些密钥

### 1. Reddit 密钥 (REDDIT_CLIENT_ID 和 REDDIT_CLIENT_SECRET)

**步骤：**

1. 访问 https://www.reddit.com/prefs/apps
2. 登录你的 Reddit 账号
3. 滚动到底部，点击 "create another app..."
4. 填写表单：
   - **name**: 随便填，比如 "PromptCrawler"
   - **app type**: 选择 "script"
   - **description**: 随便填
   - **redirect uri**: 填 `http://localhost`
5. 点击 "create app"
6. 你会看到一个卡片，上面有：
   - **client_id**: 应用名称下方的字符串（比如 `abc123def456`）
   - **client_secret**: "secret" 后面的字符串（比如 `xyz789uvw012`）

**记录下来：**

```
REDDIT_CLIENT_ID=abc123def456
REDDIT_CLIENT_SECRET=xyz789uvw012
```

------

### 2. GitHub Token (GITHUB_TOKEN)

**步骤：**

1. 访问 https://github.com/settings/tokens
2. 登录你的 GitHub 账号
3. 点击 "Generate new token (classic)"
4. 填写信息：
   - **Note**: 随便填，比如 "PromptCrawler"
   - **Expiration**: 选择 "No expiration" 或 90 天
5. 勾选权限：
   - 找到 "repo" 部分，勾选 "public_repo"
   - 其他都不用勾
6. 滚动到底部，点击 "Generate token"
7. 复制生成的 token（一长串字符，比如 `ghp_abc123def456xyz789uvw012`）

**记录下来：**

```
GITHUB_TOKEN=ghp_abc123def456xyz789uvw012
```

------

### 3. OpenAI API Key (OPENAI_API_KEY)

**步骤：**

1. 访问 https://platform.openai.com/api-keys
2. 登录你的 OpenAI 账号
3. 点击 "Create new secret key"
4. 给它起个名字，比如 "PromptCrawler"
5. 点击 "Create secret key"
6. 复制生成的 key（比如 `sk-proj-abc123def456xyz789uvw012`）

**记录下来：**

```
OPENAI_API_KEY=sk-proj-abc123def456xyz789uvw012
```

------

## 如何配置到 Supabase

**方式 1：通过 Supabase Dashboard（推荐）**

1. 打开 [https://app.supabase.com](https://app.supabase.com/)

2. 选择你的项目

3. 左侧菜单 > Edge Functions

4. 找到 "prompt-crawler" 函数

5. 点击右上角的 "Settings" 或齿轮图标

6. 在 "Environment Variables" 部分，添加：

   ```
   REDDIT_CLIENT_ID = abc123def456
   REDDIT_CLIENT_SECRET = xyz789uvw012
   GITHUB_TOKEN = ghp_abc123def456xyz789uvw012
   OPENAI_API_KEY = sk-proj-abc123def456xyz789uvw012
   ```

7. 点击 "Save"

**方式 2：通过 CLI（如果已安装）**

创建 `.env.local` 文件：

```
REDDIT_CLIENT_ID=abc123def456
REDDIT_CLIENT_SECRET=xyz789uvw012
GITHUB_TOKEN=ghp_abc123def456xyz789uvw012
OPENAI_API_KEY=sk-proj-abc123def456xyz789uvw012
```

然后运行：

```bash
supabase functions deploy prompt-crawler
```

------

## 安全提示

⚠️ **重要：**

- 这些密钥是敏感信息，不要分享给任何人
- 不要把它们提交到 Git 仓库
- 如果不小心暴露了，立即在对应平台上删除/重新生成

------

## 成本说明

| 服务       | 免费额度                                            | 超出后       |
| ---------- | --------------------------------------------------- | ------------ |
| Reddit API | 无限制                                              | 免费         |
| GitHub API | 60 请求/小时（无 Token） 5000 请求/小时（有 Token） | 免费         |
| OpenAI     | 根据账户等级                                        | 按使用量计费 |

------

## 验证配置是否成功

配置完成后，在应用中：

1. 进入"提示词采集"页面
2. 点击"立即采集"
3. 点击"查看进度"
4. 如果看到日志在运行，说明配置成功了

如果看到错误日志，比如 "Reddit credentials not configured"，说明密钥没有正确配置。